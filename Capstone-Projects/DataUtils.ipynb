{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "serial-birmingham",
   "metadata": {},
   "source": [
    "# GETTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compound-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk.tokenize as tokenize\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "automatic-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKaggleNewsDataSet():\n",
    "    fake = pd.read_csv(\"data/Fake.csv\")\n",
    "    true = pd.read_csv(\"data/True.csv\")\n",
    "    # add a field to determine fake and real\n",
    "    fake['label'] = 0\n",
    "    true['label'] = 1\n",
    "    # combine\n",
    "    data = pd.concat([fake, true]).reset_index(drop = True)\n",
    "    # suffle to prevent bias\n",
    "    data = shuffle(data)\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adjacent-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReserachArticleNewsDataSet():\n",
    "    data_dir = \"data/research-data/\"\n",
    "    rd = pd.read_csv(data_dir+\"researcharticles.zip\", sep=',', names=[\"id\", \"url\", \"source\", \"desc\"])\n",
    "    fake = readFile(rd.loc[rd['desc'] == 'Not-Real-Other'], data_dir, 0)\n",
    "    real = readFile(rd.loc[rd['desc'] == 'Real'], data_dir, 1)\n",
    "    data = pd.concat([fake, real]).reset_index(drop = True)\n",
    "    # suffle to prevent bias\n",
    "    data = shuffle(data)\n",
    "    data = data.reset_index(drop=True)\n",
    "    data['label'] = data['label'].astype(int) \n",
    "    return data\n",
    "    \n",
    "def readFile(df, data_dir, label):\n",
    "    column_names = ['text', 'label']\n",
    "    news_data = pd.DataFrame(columns = column_names)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        txt = pd.read_csv(data_dir+row['id'], sep='\\t', quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "        news_data = news_data.append({'text':txt, 'label':label}, ignore_index=True)\n",
    "    return news_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-david",
   "metadata": {},
   "source": [
    "# DATA  SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "realistic-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "def split_data(data, labels):\n",
    "#     X_train,X_test,y_train,y_test = train_test_split(data['text'], data.target, test_size=0.2, random_state=42)\n",
    "    return train_test_split(data.text, data.label, test_size=0.2, random_state=42, shuffle=\"true\")\n",
    "\n",
    "# X_train,X_test,y_train,y_test = train_test_split(data['text'], data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def train_NB(train_data, train_labels):\n",
    "    return MultinomialNB().fit(train_data, train_labels)\n",
    "\n",
    "\n",
    "def train_random_forest(train_data, train_labels, est):\n",
    "    return RandomForestClassifier(n_estimators=est).fit(train_data, \n",
    "        train_labels)\n",
    "\n",
    "\n",
    "def test_classifier(clf, validate_data, validate_labels, str):\n",
    "    predicted = clf.predict(validate_data)\n",
    "    print(str)\n",
    "    print(np.mean(predicted == validate_labels))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-width",
   "metadata": {},
   "source": [
    "# DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-blade",
   "metadata": {},
   "source": [
    "### Case Insensitive\n",
    "### Remove Stopwords\n",
    "### Remove Punctuations\n",
    "### Lemmatization OR Stemming - Lemmarization \n",
    "### POS - parts-of-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cardiac-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "import spacy\n",
    "import re\n",
    "# Using Porter Stemmer implementation in nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "punct = set(string.punctuation)\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "def clean_data(df, columns):\n",
    "    for i, col in enumerate(columns):\n",
    "        df[col] = df[col].apply(lambda text: clean_text(text))\n",
    "\n",
    "### Lemmatization OR Stemming\n",
    "# Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological \n",
    "# analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary \n",
    "# form of a word, which is known as the lemma.\n",
    "        \n",
    "def clean_text(text):\n",
    "    text = ''.join(char.lower() for char in text if char not in punct)\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = ' '.join([lm.lemmatize(word) for word in tokens if word not in sw])\n",
    "#     text = ' '.join([(lm.lemmatize(word) and stemmer.stem(word)) for word in tokens if word not in sw])\n",
    "    return text\n",
    "\n",
    "########### BELOW METHOD IS TIME CONSUMING and NOT WORKING  ##########\n",
    "\n",
    "def clean_text_data(df, columns):\n",
    "    for i, col in enumerate(columns):\n",
    "        print (i, \",\",col)\n",
    "        # convert text to lower case\n",
    "        df[col] = df[col].str.lower()\n",
    "        # remove punctuations\n",
    "        df[col] = df[col].apply(lambda text: remove_punctuation(text))\n",
    "        # tokenize and remove stopwords\n",
    "        df[col] = df[col].apply(lambda text: remove_stopwords(text))\n",
    "\n",
    "        \n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return str(text).translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "def remove_stopwords(text):\n",
    "    remove_nltk_stopwords(text)\n",
    "#     remove_spacy_stopwords(text)\n",
    "        \n",
    "def remove_nltk_stopwords(text):\n",
    "#     text = all_data[\"text\"]\n",
    "#     stop = stopwords.words('english')\n",
    "#     return text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    sw = set(stopwords.words('english'))\n",
    "    deto = Detok()\n",
    "    \n",
    "    all_cleaned = list()\n",
    "    \n",
    "    for article in text:\n",
    "        word_tokens = word_tokenize(article) \n",
    "        all_cleaned.append(deto.detokenize(\n",
    "            [w for w in word_tokens if not w in sw]))\n",
    "    return all_cleaned\n",
    "\n",
    "def remove_spacy_stopwords(text):\n",
    "    spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "    sw = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    deto = Detok()\n",
    "\n",
    "    all_cleaned = list()\n",
    "\n",
    "    for article in text:\n",
    "        word_tokens = word_tokenize(article) \n",
    "        all_cleaned.append(deto.detokenize(\n",
    "            [w for w in word_tokens if not w in sw]))\n",
    "\n",
    "    return all_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-sword",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION\n",
    "### Word counts and Cloud\n",
    "### Frequency Distributions\n",
    "### Relevancy\n",
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-norway",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affected-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "def word_cloud(text, column):\n",
    "    all_words = ' '.join(str(text) for text in text[column])\n",
    "    wordcloud = WordCloud(width= 800, height= 500,\n",
    "                              max_font_size = 110,\n",
    "                              collocations = False).generate(all_words)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-short",
   "metadata": {},
   "source": [
    "## Count (words and ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alone-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def count_words(all_data, train_data, test_data):\n",
    "    cv = CountVectorizer()\n",
    "    cv = count_vect.fit(all_data)\n",
    "    x_train_data =  cv.transform(train_data)\n",
    "    x_test_data =  cv.transform(test_data)\n",
    "    return x_train_data, x_test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-progress",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "N-grams are simply all combinations of adjacent words or letters of length n that you can find in your source text. For example, given the word fox, all 2-grams (or “bigrams”) are fo and ox. You may also count the word boundary – that would expand the list of 2-grams to #f, fo, ox, and x#, where # denotes a word boundary.\n",
    "\n",
    "You can do the same on the word level. As an example, the hello, world! text contains the following word-level bigrams: # hello, hello world, world #.\n",
    "\n",
    "The basic point of n-grams is that they capture the language structure from the statistical point of view, like what letter or word is likely to follow the given one. The longer the n-gram (the higher the n), the more context you have to work with. Optimum length really depends on the application – if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the “general knowledge” and only stick to particular cases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alternative-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(all_data, train_data, test_data):\n",
    "    cv = CountVectorizer(ngram_range=(2,3))\n",
    "    cv = count_vect.fit(all_data)\n",
    "    x_train_data =  cv.transform(train_data)\n",
    "    x_test_data =  cv.transform(test_data)\n",
    "    return x_train_data, x_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-height",
   "metadata": {},
   "source": [
    "## Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "global-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_space = tokenize.WhitespaceTokenizer()\n",
    "def frequency(text, column, quantity):\n",
    "    all_words = ' '.join(str(text) for text in text[column])\n",
    "    token_phrase = token_space.tokenize(all_words)\n",
    "    frequency = nltk.FreqDist(token_phrase)\n",
    "    df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()),\n",
    "                                   \"Frequency\": list(frequency.values())})\n",
    "    df_frequency = df_frequency.nlargest(columns = \"Frequency\", n = quantity)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = sns.barplot(data = df_frequency, x = \"Word\", y = \"Frequency\", color = 'blue')\n",
    "    ax.set(ylabel = \"Count\")\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-threat",
   "metadata": {},
   "source": [
    "## Relevancy (TF-IDF)\n",
    "TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "posted-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_words(all_data, train_data, test_data):\n",
    "    tfidfVector = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "    tfidfVector.fit(all_data)\n",
    "    x_train_data =  tfidfVector.transform(train_data)\n",
    "    x_test_data =  tfidfVector.transform(test_data)\n",
    "    return x_train_data, x_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "extraordinary-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_ngrams(all_data, train_data, test_data):\n",
    "    tfidfVector = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                                 ngram_range=(2,3), max_features=5000)\n",
    "    tfidfVector.fit(all_data)\n",
    "    x_train_data =  tfidf_vect.transform(train_data)\n",
    "    x_test_data =  tfidf_vect.transform(test_data)\n",
    "    return x_train_data, x_test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-nursing",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "supposed-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TagLemmatize import *\n",
    "def tag_lemmatize(data_list):\n",
    "    ret_list = []\n",
    "    for d in data_list:\n",
    "        ret_list.append(tag_and_lem(d))\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-chorus",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "descending-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "def vader_score(data_list):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    ret_list = list()\n",
    "    for data in data_list:\n",
    "        ret_list.append(list(analyser.polarity_scores(data).values()))\n",
    "    return ret_list\n",
    "\n",
    "def vader_score_non_neg(article_list):\n",
    "    ret_list = list()\n",
    "    for article_vals in article_list:\n",
    "        ret_list.append([x+1 for x in article_vals])\n",
    "    return ret_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-adventure",
   "metadata": {},
   "source": [
    "## POS - parts-of-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "paperback-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_of_speech(all_data):\n",
    "    # Turn all_data into PoS\n",
    "    all_pos = list()\n",
    "    for article in all_data:\n",
    "        all_pos.append(pos_tag(word_tokenize(article)))\n",
    "\n",
    "    # Create a counter for all_pos\n",
    "    all_pos_counter = list()\n",
    "    for article in all_pos:\n",
    "        all_pos_counter.append(Counter( tag for word,  tag in article))\n",
    "\n",
    "    all_pos_count = list()\n",
    "\n",
    "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "    # Count up each PoS and giving a value of 0 to those that do not occur\n",
    "    for counter in all_pos_counter:\n",
    "        temp = list()\n",
    "        for key in tagdict:\n",
    "            temp.append(counter[key])\n",
    "        all_pos_count.append(temp)\n",
    "\n",
    "    return all_pos_count\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
